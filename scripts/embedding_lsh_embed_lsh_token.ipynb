{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5d0a57",
   "metadata": {},
   "source": [
    "# 3.1 Embedding Builder \n",
    "\n",
    "This notebook is used to sentence embeddings for each game in the dataset\n",
    "\n",
    "Inputs:\n",
    "\n",
    "data/json/game_overview_final_vol2.json\n",
    "Outputs:\n",
    "\n",
    "embeddings_FINAL.py ; a matrix of shape (n_games, embedding_dim)\n",
    "games_df_FINAL.pkl ; a pandas DataFrame with all the necessary features for posterior analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909a9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavich/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f343f2",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e37aeff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../data/json/game_overview_final_vol2.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/json/game_overview_final_vol2.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of games:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../data/json/game_overview_final_vol2.json does not exist"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_json(\"../data/json/game_overview_final_vol2.json\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Number of games:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1599f14d",
   "metadata": {},
   "source": [
    "## Build text to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8460e6c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m                 parts\u001b[38;5;241m.\u001b[39mappend(val)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(parts)\n\u001b[0;32m---> 12\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mapply(make_text, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def make_text(row):\n",
    "    parts = []\n",
    "    \n",
    "    for col in ['name', 'summary', 'genres', 'platforms', 'companies', 'keywords']:\n",
    "        if col in row:\n",
    "            val = row[col]\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                parts.append(val)\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "df['text'] = df.apply(make_text, axis=1)\n",
    "\n",
    "print(df[['game_id', 'name', 'text']].head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729698ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the release date column to an actual date time with value 0-1, 0 being the oldest game in the dataset, and 1 being the most recent \n",
    "df[\"first_release_date\"] = pd.to_datetime(df[\"first_release_date\"], errors=\"coerce\")\n",
    "\n",
    "min_date = df[\"first_release_date\"].min()\n",
    "max_date = df[\"first_release_date\"].max()\n",
    "\n",
    "date_range_days = (max_date - min_date).days\n",
    "\n",
    "def compute_recency(d):\n",
    "    if pd.isna(d):\n",
    "        return 0.5\n",
    "    \n",
    "    return (d - min_date).days/date_range_days\n",
    "df[\"recency\"] = df[\"first_release_date\"].apply(compute_recency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e1fcc",
   "metadata": {},
   "source": [
    "## Generate the embeddings and save themm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embedding model (change device to 'cuda if GPU is desired to run  the SentenceTransformer)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "#Encode to embedding\n",
    "texts = df[\"text\"].tolist()\n",
    "embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Print and normalize embeddings\n",
    "print(embeddings.shape)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "print(df.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and the dataset to use in the next methods\n",
    "np.save(\"../data/embeddings_FINAL.npy\", embeddings)\n",
    "df.to_pickle(\"../data/games_df_FINAL.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3722c",
   "metadata": {},
   "source": [
    "# LSH with embeddings \n",
    "\n",
    "This notebook builds a simple LSH index on top of precomputed embeddings and provides a recommendation function\n",
    "\n",
    "It assumes you already have run the Embedding Builder notebook and generated:\n",
    "\n",
    "games_df.pkl; a DataFrame with game metadata\n",
    "embeddings.npy; numpy array of shape (n_games, embedding_dim)\n",
    "Steps taken:\n",
    "\n",
    "Load the embeddings and game data\n",
    "Generate random hyperplanes and build LSH index\n",
    "Implement LSH query to get candidate neighbours\n",
    "Rank candidates using cosine similarity + rating + recency\n",
    "Show example recommendations for a given game index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12047321",
   "metadata": {},
   "source": [
    "## Get the imports and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5250cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# In the original script we used timing/tracemalloc for profiling.\n",
    "# In this notebook we focus on the logic and interactivity, so we skip that.\n",
    "\n",
    "df = pd.read_pickle(\"../data/games_df_FINAL.pkl\")\n",
    "embeddings = np.load(\"../data/embeddings_FINAL.npy\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Number of games:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af136a",
   "metadata": {},
   "source": [
    "## generate random hyperplanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several sets of random hyperplanes are created, each corresponding to one LSH table, and each hyperplane used to generate one bit of the hash key\n",
    "def generate_hyperplanes(num_tables, num_planes, dim, seed= 0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    hyperplanes = []\n",
    "    for _ in range(num_tables):\n",
    "        planes = rng.randn(num_planes, dim)\n",
    "        planes /= np.linalg.norm(planes, axis = 1, keepdims = True) +1e-9\n",
    "        hyperplanes.append(planes)\n",
    "    \n",
    "    return hyperplanes\n",
    "\n",
    "# check it\n",
    "dim = embeddings.shape[1]\n",
    "num_tables = 10\n",
    "num_planes = 16\n",
    "\n",
    "hyperplanes = generate_hyperplanes(num_tables, num_planes, dim, seed=33)\n",
    "print(hyperplanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42161",
   "metadata": {},
   "source": [
    "## Hash each embedding into buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_v(v, planes):\n",
    "    projections = planes @ v\n",
    "    bits = projections > 0\n",
    "    key = ''.join('1' if b else '0' for b in bits)\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229658c6",
   "metadata": {},
   "source": [
    "## Get LSH index, which is tables with buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build one hash table per hyperplane set. Each table maps a bitstring key to a bucket of game indices whose embedding fall into that region\n",
    "\n",
    "def build_lsh_idx(embeddings, hyperplanes):\n",
    "    n, dim = embeddings.shape\n",
    "    num_tables = len(hyperplanes)\n",
    "\n",
    "    tables = [defaultdict(set) for _ in range(num_tables)]\n",
    "\n",
    "\n",
    "    for idx, v in enumerate(embeddings):\n",
    "        for t, planes in enumerate(hyperplanes):\n",
    "            key = hash_v(v, planes)\n",
    "            tables[t][key].add(idx)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# check it\n",
    "tables = build_lsh_idx(embeddings, hyperplanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d995d9",
   "metadata": {},
   "source": [
    "## query LSH; get candidate neighbours for a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a game idx, we compute its hash key in each table, collect all indices in the corresponding bucket, union them into a candidate set\n",
    "def lsh_query(idx, embeddings, hyperplanes, tables):\n",
    "    v = embeddings[idx]\n",
    "    candidates = set()\n",
    "\n",
    "    for t, planes in enumerate(hyperplanes):\n",
    "        key = hash_v(v, planes)\n",
    "        bucket = tables[t].get(key, set())\n",
    "        candidates |= bucket\n",
    "\n",
    "    candidates.discard(idx)\n",
    "    return candidates\n",
    "\n",
    "#test\n",
    "cand = lsh_query(0, embeddings, hyperplanes, tables)\n",
    "print(\"Candidates for game 0:\", cand)\n",
    "for candidate in cand:\n",
    "    print(df.loc[candidate, 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bce71a",
   "metadata": {},
   "source": [
    "## Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the candidates for a given game, the cosine similarity between the candidates and the game is computed and normalized to [0,1]. \n",
    "# Combining similarity, rating and recency, a final score is obtained for each candidate (0-1) and the results are sorted to print the top recommendations based on this score.\n",
    "# The weights to similarity, rating and recency can be changed to emphasize one or the other. \n",
    "def recommend_lsh_embedding(idx, embeddings, hyperplanes, tables, df, top=5):\n",
    "    v = embeddings[idx]\n",
    "    cand = lsh_query(idx, embeddings, hyperplanes, tables)\n",
    "\n",
    "    if not cand:\n",
    "        print(f\"No candidates for {df.loc[idx, 'name']}\")\n",
    "        return []\n",
    "\n",
    "    cand_indices = np.array(list(cand), dtype=int)\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "        v.reshape(1, -1),\n",
    "        embeddings[cand_indices]\n",
    "    )  # shape: (1, n_candidates)\n",
    "\n",
    "    sim_flat = sim[0]  \n",
    "    cos_norm = (sim_flat + 1.0) / 2.0\n",
    "\n",
    "\n",
    "    ratings = df.loc[cand_indices, 'steam_rating'].astype(float).to_numpy()\n",
    "    recency = df.loc[cand_indices, 'recency'].astype(float).to_numpy()\n",
    "\n",
    "\n",
    "    w_sim = 0.7\n",
    "    w_rating = 0.2\n",
    "    w_date = 0.1\n",
    "\n",
    "    final_score = (w_sim*cos_norm + w_rating*ratings + w_date*recency)\n",
    "    \n",
    "    order = np.argsort(final_score)[::-1]\n",
    "    ordered_indices = cand_indices[order]\n",
    "    ordered_cos_norm = cos_norm[order]    \n",
    "    ordered_rating = ratings[order]\n",
    "    ordered_recency = recency[order]\n",
    "    ordered_score = final_score[order]\n",
    "\n",
    "    results = list(zip(\n",
    "        ordered_indices[:top], \n",
    "        ordered_cos_norm[:top],\n",
    "        ordered_rating[:top],\n",
    "        ordered_recency[:top],\n",
    "        ordered_score[:top]))\n",
    "\n",
    "    print(f\"Query: {df.loc[idx, 'name']}\")\n",
    "    print(\"Recommendations (cosine_norm, rating, recency, score):\")\n",
    "\n",
    "    for j, cos_s, r, rec, score in results:\n",
    "        print(\n",
    "            f\" --> {df.loc[j, 'name']} \"\n",
    "            f\"(cos={cos_s:.3f}, rating={r:.3f}, recency = {rec:.3f}, score={score:.3f})\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "#test\n",
    "recommend_lsh_embedding(0, embeddings, hyperplanes, tables, df, top=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73c60c",
   "metadata": {},
   "source": [
    "# LSH with tokens \n",
    "This notebook implements an LSH-based recommender using token-shingles and MinHash signatures\n",
    "\n",
    "It:\n",
    "\n",
    "Loads the raw game dataset\n",
    "builds a text field per game\n",
    "Creates word shingles\n",
    "Computes MinHash signatures\n",
    "Builds LSH using banding\n",
    "Finds candidate similar games and ranks them using: Jaccard similarity, rating and recency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary packages\n",
    "import pandas as pd\n",
    "import mmh3\n",
    "from  collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b45efd",
   "metadata": {},
   "source": [
    "## Get the text from the dataset and build the text to shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text so that no \"weird\" shingles are formed later\n",
    "df = pd.read_json(\"../data/json/game_overview_final_vol2.json\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.replace(\";\", \" \")         \n",
    "    text = re.sub(r\"[^a-z0-9\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def make_text(row):\n",
    "    parts =[]\n",
    "\n",
    "    for col in ['name', 'summary', 'genres', 'platforms', 'companies', 'keywords']:\n",
    "        if col in row:\n",
    "            val = row[col]\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                parts.append(val)\n",
    "    \n",
    "    return \" \".join(parts)\n",
    "\n",
    "df['text'] = df.apply(make_text, axis = 1)\n",
    "\n",
    "print(df[[\"game_id\", \"name\", \"text\"]].head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b7d16",
   "metadata": {},
   "source": [
    "## Compute a normalized recency score [0, 1] from the release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"first_release_date\"] = pd.to_datetime(df[\"first_release_date\"], errors=\"coerce\")\n",
    "\n",
    "min_date = df[\"first_release_date\"].min()\n",
    "max_date = df[\"first_release_date\"].max()\n",
    "\n",
    "date_range_days = (max_date - min_date).days\n",
    "\n",
    "def compute_recency(d):\n",
    "    if pd.isna(d):\n",
    "        return 0.5\n",
    "    \n",
    "    return (d - min_date).days/date_range_days\n",
    "df[\"recency\"] = df[\"first_release_date\"].apply(compute_recency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d56142",
   "metadata": {},
   "source": [
    "## q - shingles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb429da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We turn each game's text into a set of word shingles. These shingles are the input to the MinHash function.\n",
    "def shingle(text: str, q: int = 2):\n",
    "    text = normalize_text(text)\n",
    "    words = text.split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - q + 1):\n",
    "        sh = \" \".join(words[i:i+q])\n",
    "        shingles.add(sh)\n",
    "    return shingles\n",
    "\n",
    "NUM_HASHES = 100\n",
    "BANDS = 50\n",
    "ROWS_PER_BAND = 2\n",
    "\n",
    "# Checking it works\n",
    "print(df.loc[1, \"text\"])\n",
    "print(shingle(df.loc[1, \"text\"], q=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88162a",
   "metadata": {},
   "source": [
    "## Compute a Minhash signature of length \"num_hases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8942286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash_sign(shingles, num_hashes = NUM_HASHES):\n",
    "    signature = []\n",
    "    for seed in range(num_hashes):\n",
    "        min_val = None\n",
    "        for sh in shingles:\n",
    "            h = mmh3.hash(sh, seed, signed=False)\n",
    "            if (min_val is None) or (h < min_val):\n",
    "                min_val = h\n",
    "        signature.append(min_val)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be48ec",
   "metadata": {},
   "source": [
    "## get the signature for each game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sign(df, q=2, num_hashes = NUM_HASHES):\n",
    "    signatures = {}\n",
    "    shingle_cache = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        shingles = shingle(text, q = q)\n",
    "        shingle_cache[idx] = shingles\n",
    "        sig = minhash_sign(shingles, num_hashes= num_hashes)\n",
    "        signatures[idx] = sig\n",
    "\n",
    "    return signatures, shingle_cache\n",
    "\n",
    "signatures, shingle_cache = compute_sign(df, q= 2, num_hashes= NUM_HASHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca39ac",
   "metadata": {},
   "source": [
    "## Get LSH bands and buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a90ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lsh_idx(signatures, bands=BANDS, row_band=ROWS_PER_BAND):\n",
    "    buckets = defaultdict(set)\n",
    "    for doc_id, sig in signatures.items():\n",
    "        assert len(sig) == bands * row_band\n",
    "        for b in range(bands):\n",
    "            start = b * row_band\n",
    "            end = start + row_band\n",
    "            band_slice = tuple(sig[start:end])\n",
    "            band_hash = hash(band_slice)\n",
    "            buckets[(b, band_hash)].add(doc_id)\n",
    "    return buckets\n",
    "\n",
    "buckets = build_lsh_idx(signatures, bands=BANDS, row_band= ROWS_PER_BAND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634c1fe",
   "metadata": {},
   "source": [
    "## Get candidate neighbours for a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e104fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first retrieve candidates that share at least one band with the query game\n",
    "def lsh_candidates(doc_id, signatures, buckets, bands=BANDS, row_band=ROWS_PER_BAND):\n",
    "    sig = signatures[doc_id]\n",
    "    candidates = set()\n",
    "    for b in range(bands):\n",
    "        start = b * row_band\n",
    "        end = start + row_band\n",
    "        band_slice = tuple(sig[start:end])\n",
    "        band_hash = hash(band_slice)\n",
    "        bucket_docs = buckets.get((b, band_hash), set())\n",
    "\n",
    "        for other in bucket_docs:\n",
    "            if other != doc_id:\n",
    "                candidates.add(other)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Check it on one game\n",
    "doc = 0\n",
    "candidates = lsh_candidates(doc, signatures, buckets)\n",
    "print(f\"Candidates for game 0: {candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e313a",
   "metadata": {},
   "source": [
    "## Calculate the Jaccard Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(sig1, sig2):\n",
    "    total = sum(1 for a, b in zip(sig1, sig2) if a == b)\n",
    "    return total/len(sig1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fdc76",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given game idx:\n",
    "# - Get the candidates\n",
    "# - Compute similarity of signatures\n",
    "# - Filter by a minimum similarity threshold\n",
    "# - Combine similarity, rating and recency into a final score (0-1)\n",
    "# Sort the  final score and print the top results. The weights or threshold can be changed into what wants to be emphasized. \n",
    "def similar_games(doc_id, \n",
    "                  df, \n",
    "                  signatures,\n",
    "                  buckets, \n",
    "                  bands=BANDS, \n",
    "                  row_band=ROWS_PER_BAND, \n",
    "                  thresh = 0.6,\n",
    "                  w_sim = 0.7,\n",
    "                  w_rating = 0.2,\n",
    "                  w_date = 0.1,\n",
    "                  rating_col = 'steam_rating',\n",
    "                  recency_col = 'recency'):\n",
    "    sig0 = signatures[doc_id]\n",
    "    cand = lsh_candidates(doc_id, signatures, buckets, bands, row_band)\n",
    "\n",
    "    results = []\n",
    "    for other in cand:\n",
    "        sim = jaccard_sim(sig0, signatures[other])\n",
    "\n",
    "        if sim < thresh:\n",
    "            continue\n",
    "        \n",
    "        rating = df.loc[other, rating_col]\n",
    "        recency = df.loc[other, recency_col]\n",
    "        \n",
    "        score = (\n",
    "            w_sim*sim + w_rating*rating + w_date*recency\n",
    "        )\n",
    "\n",
    "        results.append((other, sim, rating, recency, score))\n",
    "\n",
    "    results.sort(key=lambda x:x[4], reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "#test it in one case:\n",
    "idx = 0\n",
    "res = similar_games(idx, df, signatures, buckets, bands=BANDS, row_band=ROWS_PER_BAND, thresh=0.2)\n",
    "print(\"Query:\", df.loc[idx, \"name\"])\n",
    "print(\"Found\", len(res), \"similar games\")\n",
    "\n",
    "for other, sim, rating, recency, score in res[:10]:\n",
    "    print(\n",
    "        \" -->\", df.loc[other, \"name\"],\n",
    "        f\"(Jaccard={sim:.2f}, rating={rating:.2f}, recency = {recency:.2f}, score={score:.2f})\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
